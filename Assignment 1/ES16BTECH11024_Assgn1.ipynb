{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center\"> Applied Machine Learning </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <div style=\"text-align: right\"> Udit Maniyar<br><br> ES16BTECH11024 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) What happens to the training error (using all available data) when the neighbor size k varies from n to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the entire training set is taken into account,\n",
    "- If k = 1 then the closes neighbour to any point is itself and hence training error is 0\n",
    "- When k = n the majority class is given to every point, Since it is given that n/2 points belong to each class then no matter what we do half the points will be wrongly predicted to the majority class and hence the training error will be 50%  \n",
    "- In general it can be said that the loss is expected to have an increasing trend as k increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Predict and explain with a sketch how the generalization error (e.g. holding out some data for testing) would change when k varies? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If k is too small, then the generalization error will be very high because we become more sensitive to the noise in the data.\n",
    "- If k is too large and if we assume that the test data also contains equal number of points from both the classes then again the generalization error for this will be very large because majority class in the training data will be predicted always for every point in test data\n",
    "- This means we have parabolic graph for generalization error.\n",
    "- The exact variation of the test loss will depend on the similarities of distribution of the training and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1B.jpg\" alt=\"k vs Generalization Error\" style=\"width:300px;height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Give two reasons (with sound justification) why k-NN may be undesirable when the input dimension is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the dimensionality is too high, the time taken to compute the distance between the two points will be very high, considering that we need to calculate distance for each point in training set, and also store them.\n",
    "- If the dimensionality is high, the point in the space become more sparse. This means the distance between the neigbours gets increased which influences the class of the point. This is known as curse of dimensionality. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Is it possible to build a univariate decision tree which classifies exactly similar to a 1-NN using the Euclidean distance measure ? If so, explain how. If not, explain why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The decision boundary for 1-Nearest Neighbours are Voronoi Diagrams\n",
    "- The decision boundary of a decision tree are lines parallel to the axis.\n",
    "- Hence it is not possible to build Univariate decision tree which  classifies exaclty similar to 1-NN unless the the boundaries in the voronoi Diagrams are parallel to axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) A training set consists of one dimensional examples from two classes. The training examples from class 1 are {0.5, 0.1, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.35, 0.25} and from class 2 are {0.9, 0.8, 0.75, 1.0}. Fit a (one dimensional) Gaussian using Maximum Likelihood to each of these two classes. You can assume that the variance for class 1 is 0.0149, and the variance for class 2 is 0.0092. Also estimate the class probabilities p1 and p2 using Maximum Likelihood. What is the probability that the test point x = 0.6 belongs to class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2a_1.jpg\" alt=\"Page 1\" style=\"width:800px;height:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2a_2.jpg\" alt=\"Page 1\" style=\"width:800px;height:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) b Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2b.jpg\" alt=\"Page 1\" style=\"width:1000px;height:800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages Imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# Enter You Name Here\n",
    "myname = \"Udit Maniyar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    tree = {}\n",
    "\n",
    "\n",
    "    #Splits the data to two np arrays left and right based on the attribute and threshold\n",
    "    def data_split(self,training_set,attr,thresh):\n",
    "        left = []\n",
    "        right = []\n",
    "        \n",
    "        for i in training_set:\n",
    "            \n",
    "            if i[attr] <= thresh:\n",
    "                left.append(i)\n",
    "            \n",
    "            else:\n",
    "                right.append(i)\n",
    "        \n",
    "        left = np.array(left)\n",
    "        right = np.array(right)\n",
    "        return left,right\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Entropy calculation where left array contains l_one one's l_zero zero's and so on for right array\n",
    "    def entropy_change(self, l_one,l_zero,r_one,r_zero):\n",
    "        \n",
    "        #Calculation for left array\n",
    "        tot_left = l_one+l_zero\n",
    "        left_part = 0\n",
    "        \n",
    "        if tot_left!=0:\n",
    "        \n",
    "            if l_one!=0:\n",
    "                left_part += (l_one/tot_left)*math.log(l_one/tot_left)\n",
    "            \n",
    "            if l_zero!=0:\n",
    "                left_part += (l_zero/tot_left)*math.log(l_zero/tot_left)\n",
    "\n",
    "\n",
    "        #Calculation for left array\n",
    "        tot_right = r_one + r_zero\n",
    "        right_part = 0\n",
    "        \n",
    "        if tot_right!=0:\n",
    "        \n",
    "            if r_one!=0:\n",
    "                right_part += (r_one/tot_right)*math.log(r_one/tot_right)\n",
    "            \n",
    "            if r_zero!=0:\n",
    "                right_part += (r_zero/tot_right)*math.log(r_zero/tot_right)\n",
    "\n",
    "\n",
    "                \n",
    "        #Weighted Average\n",
    "        total_length = tot_left + tot_right\n",
    "        \n",
    "        left_part = (tot_left/total_length) * left_part\n",
    "        \n",
    "        right_part = (tot_right/total_length) * right_part\n",
    "\n",
    "        return -(left_part+right_part)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Ginni Index calculation where left array contains l_one one's l_zero zero's and so on for right array\n",
    "    def calc_ginni(self, l_one,l_zero,r_one,r_zero):\n",
    "        print(\"GINI\")\n",
    "        #Calculation for left array\n",
    "        tot_left = l_one+l_zero\n",
    "        left_part = 0\n",
    "        \n",
    "        if tot_left!=0:\n",
    "        \n",
    "            left_part += (l_one/tot_left)**2\n",
    "            left_part += (l_zero/tot_left)**2\n",
    "        \n",
    "        left_part = 1-left_part\n",
    "\n",
    "        \n",
    "        #Calculation for left array\n",
    "        tot_right = r_one + r_zero\n",
    "        right_part = 0\n",
    "        \n",
    "        if tot_right!=0:\n",
    "        \n",
    "            right_part += (r_one/tot_right)**2\n",
    "            right_part += (r_zero/tot_right)**2\n",
    "        \n",
    "        right_part = 1-right_part;\n",
    "\n",
    "        \n",
    "        #Weighted Average\n",
    "        total = tot_left + tot_right\n",
    "        \n",
    "        left_part = (tot_left/total) * left_part\n",
    "        \n",
    "        right_part = (tot_right/total) * right_part\n",
    "\n",
    "        return (left_part+right_part)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Selecting attributes based on entropy calculations\n",
    "    def select_attribute(self,training_set,attributes_remaining,criterion = \"entropy\",min_entropy = 0):\n",
    "\n",
    "        training_set = np.transpose(training_set)\n",
    "\n",
    "        best_entropy = math.inf\n",
    "        \n",
    "        best_attr = []\n",
    "        \n",
    "        #For each Attribute in attributes remaining, pick the best attribute\n",
    "        for i in attributes_remaining:\n",
    "        \n",
    "            attr_column = np.column_stack((training_set[i],training_set[-1]))\n",
    "            attr_column = attr_column[np.argsort(attr_column[:, 0])]\n",
    "            \n",
    "            \n",
    "            total_ones = sum(attr_column[:,-1])\n",
    "            total_zeros = len(attr_column[:,-1])-total_ones\n",
    "\n",
    "            if criterion == \"ginni-index\":\n",
    "                ith_best = self.calc_ginni(total_ones,total_zeros,0,0)\n",
    "                idx = math.inf\n",
    "            else:\n",
    "                ith_best  = self.entropy_change(total_ones,total_zeros,0,0)\n",
    "                idx = math.inf\n",
    "\n",
    "            \n",
    "            ones_tillnow = 0\n",
    "            zeros_tillnow = 0\n",
    "            \n",
    "            #In the ith attribute pick the best possible threshold\n",
    "            for j in range(len(attr_column)):\n",
    "                \n",
    "                if attr_column[j][1]==0:\n",
    "                    zeros_tillnow += 1\n",
    "                \n",
    "                else:\n",
    "                    ones_tillnow +=1\n",
    "                \n",
    "                if criterion == \"ginni-index\":\n",
    "                    current = self.calc_ginni(ones_tillnow, zeros_tillnow, total_ones-ones_tillnow,total_zeros-zeros_tillnow)\n",
    "                else:\n",
    "                    current  = self.entropy_change(ones_tillnow, zeros_tillnow, total_ones-ones_tillnow,total_zeros-zeros_tillnow)\n",
    "                   \n",
    "                \n",
    "                if current < ith_best:\n",
    "                \n",
    "                    ith_best = current\n",
    "                    idx = j\n",
    "\n",
    "                    \n",
    "            if ith_best < best_entropy:\n",
    "                  \n",
    "                    best_entropy = ith_best\n",
    "                    \n",
    "                    if idx!=math.inf:\n",
    "                        best_attr = [i,attr_column[idx][0]]\n",
    "                    \n",
    "                    else:\n",
    "                        best_attr = [i,idx]\n",
    "            if best_entropy <= min_entropy:\n",
    "                best_attr = [0,math.inf]\n",
    "\n",
    "        return best_attr\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Function call to build the tree and return the final tree in the form of a dicitonary \n",
    "    def build_tree(self,training_set,attributes_remaining,criterion,min_entropy):\n",
    "\n",
    "        #Finding the best attribute and the corresponding threshold\n",
    "        attr,thresh = self.select_attribute(training_set,attributes_remaining,criterion,min_entropy)\n",
    "        \n",
    "        #Split the data into two arrays left and right arrays based on the attr and threshold\n",
    "        left,right = self.data_split(training_set,attr,thresh)\n",
    "\n",
    "        #If the length of either left or right is 0 this means we cannot split it further we just return the most popular value\n",
    "        if len(left)==0 or len(right)==0:\n",
    "            \n",
    "            nonzero = np.count_nonzero(training_set[:,-1])\n",
    "            \n",
    "            zeros = len(training_set[:,-1]) - nonzero\n",
    "\n",
    "            if zeros >= nonzero:\n",
    "                return 0.0\n",
    "            else:\n",
    "                return 1.0\n",
    "\n",
    "        \n",
    "        #Build Left Subtree and get the left subtree in left_tree\n",
    "        left_tree = self.build_tree(left,attributes_remaining,criterion,min_entropy)\n",
    "        \n",
    "        #Build Right Subtree and get the right subtree in right_tree        \n",
    "        right_tree = self.build_tree(right,attributes_remaining,criterion,min_entropy)\n",
    "\n",
    "        \n",
    "        #Store left subtree, right subtree, attribute selected at this node and threshold in a dictionary\n",
    "        final = {\"left\":left_tree,\"right\": right_tree,\"attr\": attr,\"thresh\":thresh}\n",
    "\n",
    "        return final\n",
    "\n",
    "\n",
    "    #Calls buildtree function which returns the final tree \n",
    "    def learn(self, training_set,criterion,min_entropy):\n",
    "\n",
    "        training_set = np.array(training_set)\n",
    "\n",
    "        attributes_remaining = np.array(range(0,training_set.shape[1]-1))\n",
    "\n",
    "        self.tree = self.build_tree(training_set,attributes_remaining,criterion,min_entropy)\n",
    "\n",
    "    #Given a test instance we recursively go to left or right based on the attribute and threshold at the node\n",
    "    def find_ans(self,node,test_instance):\n",
    "\n",
    "        #Base Case\n",
    "        if type(node)==float:\n",
    "            return node\n",
    "\n",
    "        #Recurse Down the tree\n",
    "        if type(node)==dict:\n",
    "            \n",
    "            attr = node[\"attr\"]\n",
    "            thresh = node[\"thresh\"]\n",
    "\n",
    "            #Depending on the threhold value either go left or right\n",
    "            if test_instance[attr] <=thresh:\n",
    "                return self.find_ans(node[\"left\"],test_instance)\n",
    "            else:\n",
    "                return self.find_ans(node[\"right\"],test_instance)\n",
    "\n",
    "\n",
    "    #Calls find ans which recursively finds the ans\n",
    "    def classify(self, test_instance):\n",
    "        \n",
    "        result = self.find_ans(self.tree,test_instance)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) 10 - Fold Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function which runs the decision tree\n",
    "def run_decision_tree(criterion,min_entropy = 0):\n",
    "\n",
    "    # Load data set\n",
    "    with open(\"wine-dataset.csv\") as f:\n",
    "        next(f, None)\n",
    "        data = [tuple(map(float, line)) for line in csv.reader(f, delimiter=\",\")]\n",
    "    print (\"Number of records: %d\" % len(data))\n",
    "\n",
    "    #Generate a random permutation of the data set\n",
    "    data = np.random.permutation(data)\n",
    "\n",
    "    final_accuracy = 0\n",
    "    \n",
    "    # Split training/test sets\n",
    "    for j in range(9,10):\n",
    "        \n",
    "        K = 10\n",
    "        \n",
    "        training_set = [x for i, x in enumerate(data) if i % K != j]\n",
    "        \n",
    "        test_set = [x for i, x in enumerate(data) if i % K == j]\n",
    "        \n",
    "        tree = DecisionTree()\n",
    "\n",
    "        # Construct a tree using training set\n",
    "        tree.learn( training_set, criterion, min_entropy)\n",
    "        \n",
    "        \n",
    "        # Classify the test set using the tree we just constructed\n",
    "        results = []\n",
    "        for instance in test_set:\n",
    "        \n",
    "            result = tree.classify( instance[:-1] )\n",
    "            \n",
    "            results.append( result == float(instance[-1]))\n",
    "\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy = float(results.count(True))/float(len(results))\n",
    "        \n",
    "        final_accuracy+=accuracy\n",
    "        \n",
    "        print(\"accuracy: %.4f\" % accuracy)\n",
    "\n",
    "    \n",
    "    final_accuracy = final_accuracy/10\n",
    "    print(\"Final_Accuracy : %.4f\" % final_accuracy )\n",
    "    \n",
    "    # Writing results to a file (DO NOT CHANGE)\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "4409\n",
      "yes\n",
      "2856\n",
      "yes\n",
      "1219\n",
      "yes\n",
      "209\n",
      "yes\n",
      "123\n",
      "yes\n",
      "86\n",
      "yes\n",
      "32\n",
      "yes\n",
      "54\n",
      "yes\n",
      "32\n",
      "yes\n",
      "8\n",
      "yes\n",
      "24\n",
      "yes\n",
      "22\n",
      "yes\n",
      "15\n",
      "yes\n",
      "7\n",
      "yes\n",
      "1010\n",
      "yes\n",
      "560\n",
      "yes\n",
      "463\n",
      "yes\n",
      "324\n",
      "yes\n",
      "155\n",
      "yes\n",
      "27\n",
      "yes\n",
      "128\n",
      "yes\n",
      "38\n",
      "yes\n",
      "32\n",
      "yes\n",
      "29\n",
      "yes\n",
      "20\n",
      "yes\n",
      "18\n",
      "yes\n",
      "2\n",
      "yes\n",
      "9\n",
      "yes\n",
      "3\n",
      "yes\n",
      "6\n",
      "yes\n",
      "90\n",
      "yes\n",
      "57\n",
      "yes\n",
      "47\n",
      "yes\n",
      "20\n",
      "yes\n",
      "9\n",
      "yes\n",
      "11\n",
      "yes\n",
      "3\n",
      "yes\n",
      "8\n",
      "yes\n",
      "27\n",
      "yes\n",
      "10\n",
      "yes\n",
      "4\n",
      "yes\n",
      "6\n",
      "yes\n",
      "33\n",
      "yes\n",
      "169\n",
      "yes\n",
      "100\n",
      "yes\n",
      "13\n",
      "yes\n",
      "87\n",
      "yes\n",
      "63\n",
      "yes\n",
      "55\n",
      "yes\n",
      "7\n",
      "yes\n",
      "48\n",
      "yes\n",
      "39\n",
      "yes\n",
      "2\n",
      "yes\n",
      "37\n",
      "yes\n",
      "36\n",
      "yes\n",
      "6\n",
      "yes\n",
      "30\n",
      "yes\n",
      "1\n",
      "yes\n",
      "9\n",
      "yes\n",
      "6\n",
      "yes\n",
      "3\n",
      "yes\n",
      "8\n",
      "yes\n",
      "24\n",
      "yes\n",
      "19\n",
      "yes\n",
      "15\n",
      "yes\n",
      "4\n",
      "yes\n",
      "5\n",
      "yes\n",
      "69\n",
      "yes\n",
      "66\n",
      "yes\n",
      "53\n",
      "yes\n",
      "50\n",
      "yes\n",
      "39\n",
      "yes\n",
      "32\n",
      "yes\n",
      "21\n",
      "yes\n",
      "3\n",
      "yes\n",
      "18\n",
      "yes\n",
      "14\n",
      "yes\n",
      "4\n",
      "yes\n",
      "11\n",
      "yes\n",
      "7\n",
      "yes\n",
      "11\n",
      "yes\n",
      "3\n",
      "yes\n",
      "13\n",
      "yes\n",
      "3\n",
      "yes\n",
      "139\n",
      "yes\n",
      "128\n",
      "yes\n",
      "37\n",
      "yes\n",
      "3\n",
      "yes\n",
      "34\n",
      "yes\n",
      "91\n",
      "yes\n",
      "11\n",
      "yes\n",
      "97\n",
      "yes\n",
      "56\n",
      "yes\n",
      "49\n",
      "yes\n",
      "2\n",
      "yes\n",
      "47\n",
      "yes\n",
      "7\n",
      "yes\n",
      "41\n",
      "yes\n",
      "27\n",
      "yes\n",
      "14\n",
      "yes\n",
      "6\n",
      "yes\n",
      "8\n",
      "yes\n",
      "450\n",
      "yes\n",
      "302\n",
      "yes\n",
      "106\n",
      "yes\n",
      "22\n",
      "yes\n",
      "19\n",
      "yes\n",
      "3\n",
      "yes\n",
      "84\n",
      "yes\n",
      "196\n",
      "yes\n",
      "151\n",
      "yes\n",
      "144\n",
      "yes\n",
      "4\n",
      "yes\n",
      "140\n",
      "yes\n",
      "33\n",
      "yes\n",
      "107\n",
      "yes\n",
      "25\n",
      "yes\n",
      "82\n",
      "yes\n",
      "34\n",
      "yes\n",
      "29\n",
      "yes\n",
      "25\n",
      "yes\n",
      "3\n",
      "yes\n",
      "22\n",
      "yes\n",
      "4\n",
      "yes\n",
      "5\n",
      "yes\n",
      "48\n",
      "yes\n",
      "3\n",
      "yes\n",
      "45\n",
      "yes\n",
      "7\n",
      "yes\n",
      "45\n",
      "yes\n",
      "148\n",
      "yes\n",
      "40\n",
      "yes\n",
      "21\n",
      "yes\n",
      "14\n",
      "yes\n",
      "7\n",
      "yes\n",
      "3\n",
      "yes\n",
      "4\n",
      "yes\n",
      "19\n",
      "yes\n",
      "15\n",
      "yes\n",
      "10\n",
      "yes\n",
      "5\n",
      "yes\n",
      "4\n",
      "yes\n",
      "108\n",
      "yes\n",
      "57\n",
      "yes\n",
      "22\n",
      "yes\n",
      "7\n",
      "yes\n",
      "15\n",
      "yes\n",
      "6\n",
      "yes\n",
      "4\n",
      "yes\n",
      "2\n",
      "yes\n",
      "9\n",
      "yes\n",
      "35\n",
      "yes\n",
      "51\n",
      "yes\n",
      "37\n",
      "yes\n",
      "10\n",
      "yes\n",
      "27\n",
      "yes\n",
      "22\n",
      "yes\n",
      "6\n",
      "yes\n",
      "16\n",
      "yes\n",
      "11\n",
      "yes\n",
      "8\n",
      "yes\n",
      "3\n",
      "yes\n",
      "5\n",
      "yes\n",
      "5\n",
      "yes\n",
      "14\n",
      "yes\n",
      "1637\n",
      "yes\n",
      "1418\n",
      "yes\n",
      "366\n",
      "yes\n",
      "330\n",
      "yes\n",
      "159\n",
      "yes\n",
      "52\n",
      "yes\n",
      "4\n",
      "yes\n",
      "48\n",
      "yes\n",
      "107\n",
      "yes\n",
      "171\n",
      "yes\n",
      "113\n",
      "yes\n",
      "97\n",
      "yes\n",
      "16\n",
      "yes\n",
      "12\n",
      "yes\n",
      "4\n",
      "yes\n",
      "58\n",
      "yes\n",
      "2\n",
      "yes\n",
      "56\n",
      "yes\n",
      "26\n",
      "yes\n",
      "17\n",
      "yes\n",
      "4\n",
      "yes\n",
      "13\n",
      "yes\n",
      "9\n",
      "yes\n",
      "4\n",
      "yes\n",
      "9\n",
      "yes\n",
      "30\n",
      "yes\n",
      "36\n",
      "yes\n",
      "13\n",
      "yes\n",
      "23\n",
      "yes\n",
      "4\n",
      "yes\n",
      "19\n",
      "yes\n",
      "1052\n",
      "yes\n",
      "150\n",
      "yes\n",
      "84\n",
      "yes\n",
      "82\n",
      "yes\n",
      "2\n",
      "yes\n",
      "66\n",
      "yes\n",
      "902\n",
      "yes\n",
      "785\n",
      "yes\n",
      "117\n",
      "yes\n",
      "105\n",
      "yes\n",
      "19\n",
      "yes\n",
      "4\n",
      "yes\n",
      "15\n",
      "yes\n",
      "86\n",
      "yes\n",
      "12\n",
      "yes\n",
      "2\n",
      "yes\n",
      "10\n",
      "yes\n",
      "219\n",
      "yes\n",
      "161\n",
      "yes\n",
      "52\n",
      "yes\n",
      "14\n",
      "yes\n",
      "38\n",
      "yes\n",
      "27\n",
      "yes\n",
      "6\n",
      "yes\n",
      "21\n",
      "yes\n",
      "10\n",
      "yes\n",
      "11\n",
      "yes\n",
      "4\n",
      "yes\n",
      "7\n",
      "yes\n",
      "11\n",
      "yes\n",
      "109\n",
      "yes\n",
      "14\n",
      "yes\n",
      "11\n",
      "yes\n",
      "3\n",
      "yes\n",
      "95\n",
      "yes\n",
      "58\n",
      "yes\n",
      "35\n",
      "yes\n",
      "17\n",
      "yes\n",
      "14\n",
      "yes\n",
      "3\n",
      "yes\n",
      "18\n",
      "yes\n",
      "6\n",
      "yes\n",
      "12\n",
      "yes\n",
      "23\n",
      "yes\n",
      "21\n",
      "yes\n",
      "2\n",
      "yes\n",
      "1553\n",
      "yes\n",
      "1234\n",
      "yes\n",
      "73\n",
      "yes\n",
      "56\n",
      "yes\n",
      "17\n",
      "yes\n",
      "8\n",
      "yes\n",
      "2\n",
      "yes\n",
      "6\n",
      "yes\n",
      "9\n",
      "yes\n",
      "1161\n",
      "yes\n",
      "738\n",
      "yes\n",
      "423\n",
      "yes\n",
      "322\n",
      "yes\n",
      "226\n",
      "yes\n",
      "34\n",
      "yes\n",
      "18\n",
      "yes\n",
      "3\n",
      "yes\n",
      "15\n",
      "yes\n",
      "16\n",
      "yes\n",
      "9\n",
      "yes\n",
      "7\n",
      "yes\n",
      "192\n",
      "yes\n",
      "15\n",
      "yes\n",
      "8\n",
      "yes\n",
      "7\n",
      "yes\n",
      "3\n",
      "yes\n",
      "4\n",
      "yes\n",
      "177\n",
      "yes\n",
      "115\n",
      "yes\n",
      "91\n",
      "yes\n",
      "16\n",
      "yes\n",
      "14\n",
      "yes\n",
      "2\n",
      "yes\n",
      "75\n",
      "yes\n",
      "42\n",
      "yes\n",
      "34\n",
      "yes\n",
      "11\n",
      "yes\n",
      "23\n",
      "yes\n",
      "16\n",
      "yes\n",
      "7\n",
      "yes\n",
      "9\n",
      "yes\n",
      "4\n",
      "yes\n",
      "5\n",
      "yes\n",
      "7\n",
      "yes\n",
      "8\n",
      "yes\n",
      "5\n",
      "yes\n",
      "3\n",
      "yes\n",
      "33\n",
      "yes\n",
      "29\n",
      "yes\n",
      "11\n",
      "yes\n",
      "7\n",
      "yes\n",
      "4\n",
      "yes\n",
      "18\n",
      "yes\n",
      "6\n",
      "yes\n",
      "12\n",
      "yes\n",
      "4\n",
      "yes\n",
      "24\n",
      "yes\n",
      "14\n",
      "yes\n",
      "10\n",
      "yes\n",
      "62\n",
      "yes\n",
      "11\n",
      "yes\n",
      "51\n",
      "yes\n",
      "13\n",
      "yes\n",
      "5\n",
      "yes\n",
      "2\n",
      "yes\n",
      "3\n",
      "yes\n",
      "8\n",
      "yes\n",
      "38\n",
      "yes\n",
      "17\n",
      "yes\n",
      "21\n",
      "yes\n",
      "7\n",
      "yes\n",
      "14\n",
      "yes\n",
      "6\n",
      "yes\n",
      "8\n",
      "yes\n",
      "96\n",
      "yes\n",
      "48\n",
      "yes\n",
      "8\n",
      "yes\n",
      "40\n",
      "yes\n",
      "34\n",
      "yes\n",
      "6\n",
      "yes\n",
      "28\n",
      "yes\n",
      "6\n",
      "yes\n",
      "48\n",
      "yes\n",
      "37\n",
      "yes\n",
      "25\n",
      "yes\n",
      "16\n",
      "yes\n",
      "13\n",
      "yes\n",
      "6\n",
      "yes\n",
      "7\n",
      "yes\n",
      "3\n",
      "yes\n",
      "9\n",
      "yes\n",
      "12\n",
      "yes\n",
      "11\n",
      "yes\n",
      "101\n",
      "yes\n",
      "18\n",
      "yes\n",
      "10\n",
      "yes\n",
      "8\n",
      "yes\n",
      "5\n",
      "yes\n",
      "3\n",
      "yes\n",
      "83\n",
      "yes\n",
      "19\n",
      "yes\n",
      "5\n",
      "yes\n",
      "14\n",
      "yes\n",
      "11\n",
      "yes\n",
      "2\n",
      "yes\n",
      "9\n",
      "yes\n",
      "8\n",
      "yes\n",
      "1\n",
      "yes\n",
      "3\n",
      "yes\n",
      "64\n",
      "yes\n",
      "32\n",
      "yes\n",
      "29\n",
      "yes\n",
      "26\n",
      "yes\n",
      "15\n",
      "yes\n",
      "11\n",
      "yes\n",
      "6\n",
      "yes\n",
      "5\n",
      "yes\n",
      "3\n",
      "yes\n",
      "2\n",
      "yes\n",
      "3\n",
      "yes\n",
      "3\n",
      "yes\n",
      "32\n",
      "yes\n",
      "9\n",
      "yes\n",
      "23\n",
      "yes\n",
      "319\n",
      "yes\n",
      "301\n",
      "yes\n",
      "89\n",
      "yes\n",
      "11\n",
      "yes\n",
      "78\n",
      "yes\n",
      "35\n",
      "yes\n",
      "3\n",
      "yes\n",
      "32\n",
      "yes\n",
      "24\n",
      "yes\n",
      "10\n",
      "yes\n",
      "14\n",
      "yes\n",
      "8\n",
      "yes\n",
      "5\n",
      "yes\n",
      "3\n",
      "yes\n",
      "6\n",
      "yes\n",
      "8\n",
      "yes\n",
      "3\n",
      "yes\n",
      "5\n",
      "yes\n",
      "43\n",
      "yes\n",
      "35\n",
      "yes\n",
      "4\n",
      "yes\n",
      "31\n",
      "yes\n",
      "5\n",
      "yes\n",
      "26\n",
      "yes\n",
      "16\n",
      "yes\n",
      "6\n",
      "yes\n",
      "10\n",
      "yes\n",
      "3\n",
      "yes\n",
      "7\n",
      "yes\n",
      "10\n",
      "yes\n",
      "8\n",
      "yes\n",
      "212\n",
      "yes\n",
      "58\n",
      "yes\n",
      "51\n",
      "yes\n",
      "11\n",
      "yes\n",
      "7\n",
      "yes\n",
      "4\n",
      "yes\n",
      "40\n",
      "yes\n",
      "11\n",
      "yes\n",
      "29\n",
      "yes\n",
      "8\n",
      "yes\n",
      "21\n",
      "yes\n",
      "10\n",
      "yes\n",
      "11\n",
      "yes\n",
      "7\n",
      "yes\n",
      "4\n",
      "yes\n",
      "7\n",
      "yes\n",
      "154\n",
      "yes\n",
      "5\n",
      "yes\n",
      "149\n",
      "yes\n",
      "2\n",
      "yes\n",
      "147\n",
      "yes\n",
      "145\n",
      "yes\n",
      "133\n",
      "yes\n",
      "127\n",
      "yes\n",
      "6\n",
      "yes\n",
      "12\n",
      "yes\n",
      "2\n",
      "yes\n",
      "18\n",
      "yes\n",
      "15\n",
      "yes\n",
      "2\n",
      "yes\n",
      "13\n",
      "yes\n",
      "3\n",
      "yes\n",
      "accuracy: 0.8405\n",
      "Final_Accuracy : 0.0840\n"
     ]
    }
   ],
   "source": [
    "run_decision_tree(\"entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) - Improvement Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8000\n",
      "accuracy: 0.8469\n",
      "accuracy: 0.8184\n",
      "accuracy: 0.8327\n",
      "accuracy: 0.8122\n",
      "accuracy: 0.8531\n",
      "accuracy: 0.8082\n",
      "accuracy: 0.8265\n",
      "accuracy: 0.8139\n",
      "accuracy: 0.8262\n",
      "Final_Accuracy : 0.8238\n"
     ]
    }
   ],
   "source": [
    "run_decision_tree(criterion = \"gini-index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gini Index and entropy measure both give almost equal accuracy on the given  data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pruning the tree using Min entropy i.e if we find any node with entropy less than minimum entropy then we make the node leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8204\n",
      "accuracy: 0.8245\n",
      "accuracy: 0.8429\n",
      "accuracy: 0.8265\n",
      "accuracy: 0.8510\n",
      "accuracy: 0.8204\n",
      "accuracy: 0.8020\n",
      "accuracy: 0.8633\n",
      "accuracy: 0.8405\n",
      "accuracy: 0.8037\n",
      "Final_Accuracy : 0.8295\n"
     ]
    }
   ],
   "source": [
    "run_decision_tree(criterion = \"entropy\",min_entropy = 0.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 4898\n",
      "accuracy: 0.8388\n",
      "accuracy: 0.8265\n",
      "accuracy: 0.8143\n",
      "accuracy: 0.8204\n",
      "accuracy: 0.8184\n",
      "accuracy: 0.8286\n",
      "accuracy: 0.8143\n",
      "accuracy: 0.8408\n",
      "accuracy: 0.8405\n",
      "accuracy: 0.8119\n",
      "Final_Accuracy : 0.8254\n"
     ]
    }
   ],
   "source": [
    "run_decision_tree(criterion = \"gini-index\",min_entropy = 0.11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4th Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle : Whats Cooking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,csv\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import neighbors\n",
    "\n",
    "#Gets input from the json file\n",
    "def get_input(filename):\n",
    "\n",
    "    # Load data set\n",
    "    with open(filename) as f:\n",
    "        data = json.loads(f.read())\n",
    "    \n",
    "    print(\"File :\",filename)\n",
    "    print (\"Number of records: %d\" % len(data))\n",
    "\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "\n",
    "#takes the data and converts it into data_set\n",
    "#Data_set is a len(training_data) * len(attributes) matrix where an element i,j is 1 if ith row in training_data has jth ingredient\n",
    "def convert_to_dataset(data):\n",
    "    attributes = {}\n",
    "    cuisines = []\n",
    "    \n",
    "    for row in range(len(data)):\n",
    "        cuisines.append(data[row][\"cuisine\"])\n",
    "        for ingredient in data[row][\"ingredients\"]:\n",
    "\n",
    "            ingredient = ingredient.lower()\n",
    "\n",
    "            if ingredient in attributes:\n",
    "                attributes[ingredient].append(row)\n",
    "            else:\n",
    "                attributes[ingredient] = [row]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    attributes_list = list(attributes.keys())\n",
    "\n",
    "    data_set = np.zeros((len(data),len(attributes)),dtype = bool)\n",
    "\n",
    "\n",
    "    for idx,(key,val) in enumerate(attributes.items()):\n",
    "        for j in val:\n",
    "            data_set[j][idx] = 1\n",
    "\n",
    "    return data_set,cuisines,attributes_list\n",
    "\n",
    "#get test data as input\n",
    "def get_testdata(attributes):\n",
    "    data = get_input(\"test.json\")\n",
    "    data_set = np.zeros((len(data),len(attributes)),dtype = bool)\n",
    "    id = []\n",
    "    for row in range(len(data)):\n",
    "        id.append(data[row][\"id\"])\n",
    "        for ingredient in data[row][\"ingredients\"]:\n",
    "            ingredient = ingredient.lower()\n",
    "\n",
    "            if ingredient in attributes:\n",
    "                pos = attributes.index(ingredient)\n",
    "                data_set[row][pos] = 1\n",
    "\n",
    "    return data_set,id\n",
    "\n",
    "#Write the output to a file named output.csv\n",
    "def write_outcome(outcome,id):\n",
    "    outcome = np.column_stack((id,outcome))\n",
    "\n",
    "    with open('outcome.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerows([['id','cuisine']])\n",
    "        writer.writerows(outcome)\n",
    "    writeFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree using scikit\n",
    "def dtree(x,y,test_data,id):\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model = model.fit(x,y)\n",
    "    outcome = model.predict(test_data)\n",
    "    write_outcome(outcome,id)\n",
    "    print(\"Decision Tree Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bernoulli bayes tree using scikit\n",
    "def naive_bayesbernoulli(x,y,test_data,id):\n",
    "    model = naive_bayes.BernoulliNB()\n",
    "    model.fit(x,y)\n",
    "    outcome = model.predict(test_data)\n",
    "    write_outcome(outcome,id)\n",
    "    print(\"Naive Bayes Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(x,y,test_data,id):\n",
    "    model = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "    model.fit(x,y)    \n",
    "    outcome = model.predict(test_data)\n",
    "    write_outcome(outcome,id)\n",
    "    print(\"K nearest neighbours completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtree_naive(x,y,test_data,id,attributes_list):\n",
    "    \n",
    "    model1 = naive_bayes.BernoulliNB()\n",
    "    model1.fit(x,y)\n",
    "    outcome1 = model1.predict(test_data)\n",
    "    prob1 = model1.predict_proba(test_data)\n",
    "    \n",
    "    model2 = tree.DecisionTreeClassifier(min_impurity_decrease = 0.0006)\n",
    "    model2 = model2.fit(x,y)\n",
    "    outcome2 = model2.predict(test_data)\n",
    "    prob2 = model2.predict_proba(test_data)\n",
    "    outcome = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        max1 = max(prob1[i])\n",
    "        max2 = max(prob2[i])\n",
    "        if max1 >= max2:\n",
    "            outcome.append(outcome1[i])\n",
    "        else:\n",
    "            outcome.append(outcome2[i])\n",
    "     \n",
    "    write_outcome(outcome,id)\n",
    "    print(\"Naive Bayes and decision Tree Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File : train.json\n",
      "Number of records: 39774\n",
      "File : test.json\n",
      "Number of records: 9944\n"
     ]
    }
   ],
   "source": [
    "data = get_input(\"train.json\")\n",
    "x, y, attributes_list= convert_to_dataset(data)\n",
    "test_data,id = get_testdata(attributes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree(x,y,test_data,id)\n",
    "#Accuracy Achieved by this Decision Tree is 0.6196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayesbernoulli(x,y,test_data,id)\n",
    "#Accuracy Achieved by this Naive Bernoulli Bayes is 0.71178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(x,y,test_data,id)\n",
    "# Accuracy Achieved Using Knn is 0.49175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_naive(x,y,test_data,id,attributes_list)\n",
    "#Accuracy achieved is 0.71550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ingredients are chosen as attributes and cuisines are chosen as classes\n",
    "- Consider a boolean matrix x in which if x [ i ][ j ] == 1 then jth attriute is present in instance\n",
    "- Another Pre-Processing step is to convert all the upper case attributes to lower cases because it doesn't matter if the attribute is in upper case letter or lower case letters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes is a probabilistic classifier that makes classifications using the Maximum A Posteriori decision rule in a Bayesian setting\n",
    "- Naive Bayes Bernoulli gives an accuracy of 0.71178"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences.\n",
    "- It is one way to display an algorithm that only contains conditional control statements.\n",
    "- Ginni Index is used to measure the quality of split.\n",
    "- Decision Tree gives an accuracy of 0.6196\n",
    "- Setting min_impurity_decrease(Pruning) to other values(0.005,0.0001) reduces the accuracy to (0.3877,0.60659) respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K nearest Neigbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In k-nearest neighbors (k-NN), the classification is achieved by majority vote in the vicinity of data.\n",
    "- Knn takes lots of time to run  ~ 2hrs\n",
    "- Accuracy achieved by Knn when we consider 3 nearest neighbours is 0.49175\n",
    "- Since the number of dimensions is very high it takes lot of time to run, this is known as curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree  + Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this model i have used both the models Decision Tree and Naive Bayes\n",
    "- We first predict an instance of training data using Naive Bayes and generate the corresponding probablities for each class\n",
    "- We do predict the same instance using Naive Bayes Bernoulli and also generate the corresponding Probabilities for each class\n",
    "- Then we choose the class predicted by NB if Probability given by Naive Bayes is greater than probability given by Decision Tree\n",
    "- Else we choose the class predicted by decision tree\n",
    "- Decision tree was implemented by setting min_impurity_decrese = 0.0006\n",
    "- The reson for pruning is if we use the model generated by decision tree on the training data itself we can see that only very few entries are being predicted wrongly this means, Decision Tree is over fitting on the training data. To induce some error into the model i have pruned the tree using min_impurity_decrese = 0.0006\n",
    "- Accuracy Achieved by this model is 0.71550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Performance was given by Decision Tree + Naive Bayes and Decision Tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
